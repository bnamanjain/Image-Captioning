# -*- coding: utf-8 -*-
"""Image Captioining

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1cmTYtjHQTjMPZ4N7w7A8nc20YU7wplc3
"""

!pip install -q git+https://github.com/huggingface/transformers.git

!pip install -q datasets

import json

!unzip /content/images-20230525T065417Z-001.zip

import numpy as np
import pandas as pd

df = pd.read_csv("/content/captions.csv")

df = df.dropna().reset_index(drop=True)

captions = []
for idx, row in df.iterrows():
    caption_dict = {}
    caption_dict['file_name'] = row['Image File'].split('/')[1]+".jpg"
    caption_dict['text'] = row['Caption']
    captions.append(caption_dict)

root = "/content/images"

with open(root +"/"+"metadata.jsonl", 'w') as f:
    for item in captions:
        f.write(json.dumps(item) + "\n")

from datasets import load_dataset 

dataset = load_dataset("imagefolder", data_dir=root, split="train")

dataset

example = dataset[10]
image = example["image"]
width, height = image.size
display(image.resize((int(0.3*width), int(0.3*height))))

example["text"]

from torch.utils.data import Dataset

class ImageCaptioningDataset(Dataset):
    def __init__(self, dataset, processor):
        self.dataset = dataset
        self.processor = processor

    def __len__(self):
        return len(self.dataset)

    def __getitem__(self, idx):
        item = self.dataset[idx]

        encoding = self.processor(images=item["image"], text=item["text"], padding="max_length", return_tensors="pt")

        # remove batch dimension
        encoding = {k:v.squeeze() for k,v in encoding.items()}

        return encoding

from transformers import AutoProcessor

processor = AutoProcessor.from_pretrained("microsoft/git-base")

train_dataset = ImageCaptioningDataset(dataset, processor)

item = train_dataset[0]
for k,v in item.items():
      print(k,v.shape)

from torch.utils.data import DataLoader

train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=2)

batch = next(iter(train_dataloader))
for k,v in batch.items():
    print(k,v.shape)

processor.decode(batch["input_ids"][0])

from PIL import Image
import numpy as np

MEAN = np.array([123.675, 116.280, 103.530]) / 255
STD = np.array([58.395, 57.120, 57.375]) / 255

unnormalized_image = (batch["pixel_values"][0].numpy() * np.array(STD)[:, None, None]) + np.array(MEAN)[:, None, None]
unnormalized_image = (unnormalized_image * 255).astype(np.uint8)
unnormalized_image = np.moveaxis(unnormalized_image, 0, -1)
Image.fromarray(unnormalized_image)

from transformers import AutoModelForCausalLM

model = AutoModelForCausalLM.from_pretrained("microsoft/git-base")

outputs = model(input_ids=batch["input_ids"],
                attention_mask=batch["attention_mask"],
                pixel_values=batch["pixel_values"],
                labels=batch["input_ids"])
outputs.loss

import torch

optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)

device = "cuda" if torch.cuda.is_available() else "cpu"
model.to(device)

model.train()

for epoch in range(20):
    
    print("Epoch:", epoch)
    for idx, batch in enumerate(train_dataloader):
        input_ids = batch.pop("input_ids").to(device)
        pixel_values = batch.pop("pixel_values").to(device)

        outputs = model(input_ids=input_ids,
                        pixel_values=pixel_values,
                        labels=input_ids)

        loss = outputs.loss

        print("Loss:", loss.item())

        loss.backward()

        optimizer.step()
        optimizer.zero_grad()

# load image
example = dataset[0]
image = example["image"]
width, height = image.size
display(image.resize((int(0.3*width), int(0.3*height))))

inputs = processor(images=image, return_tensors="pt").to(device)
pixel_values = inputs.pixel_values

generated_ids = model.generate(pixel_values=pixel_values, max_length=50)
generated_caption = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]
print(generated_caption)

example = dataset[1]
image = example["image"]
inputs = processor(images=image, return_tensors="pt").to(device)
pixel_values = inputs.pixel_values

generated_ids = model.generate(pixel_values=pixel_values, max_length=50)
generated_caption = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]
print(generated_caption)

example = dataset[2]
image = example["image"]
inputs = processor(images=image, return_tensors="pt").to(device)
pixel_values = inputs.pixel_values

generated_ids = model.generate(pixel_values=pixel_values, max_length=50)
generated_caption = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]
print(generated_caption)

example = dataset[6]
image = example["image"]
inputs = processor(images=image, return_tensors="pt").to(device)
pixel_values = inputs.pixel_values

generated_ids = model.generate(pixel_values=pixel_values, max_length=50)
generated_caption = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]
print(generated_caption)

# PATH = "/content/"
# torch.save(model.state_dict(), PATH)
# PATH = "/content/"
# torch.save(model, PATH)
torch.save(model.state_dict(), '/content/model.pth')

